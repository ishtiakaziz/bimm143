---
title: "miniprojmark"
output:
  pdf_document: default
  html_document: default
date: "2023-05-08"
---

```{r}
# save csv file to project folder
fna.data <- "WisconsinCancer.csv"
#assign to a variable
wisc.df <- read.csv(fna.data, row.names = 1)
```

```{r}
# remove expert diagnosis from data aka first column
wisc.data <- wisc.df[,-1]
#save first column for later
diagnosis <- factor(wisc.df[,1])
```

```{r}
diagnosis
```
Q1. How many observations are there?
569 observations

Q2. How many observations are malignant?
212 observations

```{r}
grep("M", diagnosis)
```

Q3. How many variables/features are suffixed with _mean?
10 variables

```{r}
#check column means and standard dev for scaling purposes
colMeans(wisc.data)

apply(wisc.data,2,sd)
```
```{r}
#perform pca on wisc.data
wisc.pr <- prcomp(wisc.data, scale=TRUE)
apply
summary(wisc.pr)
```
Q4. What proportion of the original variance is captured by PC1?
0.4427 or 44.27%

Q5. How many PCs required to describe atleast 70% of the original variance?
3 PCs are required.

Q6. How many PCs are required to describe atleast 90% of the original variance?
7 PCs are required.

```{r}
#visualizing the PCA model 
biplot(wisc.pr)
```
Q7. What stands out of this plot? 
I can't make much of this plot. It is very chaotic and full of overlapping data.

```{r}
#scatter plot observations by component 1 and 2
plot(wisc.pr$x[,1:2], col = diagnosis , 
     xlab = "PC1", ylab = "PC2")
```

```{r}
#scatter plot observations by component 1 and 3
plot(wisc.pr$x[,1-3], col = diagnosis , 
     xlab = "PC1", ylab = "PC3")
```

Q8. What do you notice about the plot for PC1/3?
There seems to be a slightly clearler distinction
between red and black as well as closer grouping.

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

```{r}
#calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```
```{r}
pve <- pr.var/sum(pr.var)

# plot variance
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

Q9. What is the component of the loading vector for concave.points_mean for PC1?
-0.26085376

Q10. Minimum number of PCs required to explain 80% of variance?
5 PCs

```{r}
#scale wisc.data
data.scaled <- scale(wisc.data)
#calculate euclidean distance between all pairs of observations in scaled data
data.dist <- dist(data.scaled)
#create a heirarchal clustering model using complete linkage
wisc.hclust <- hclust(data.dist, method = "complete")
```



```{r}
plot(wisc.hclust)
abline(wisc.hclust, col="red", lty=2)
```
Q11. At what height is there 4 clusters?
20 (a horizontal line crosses 4 time at height 20)

```{r}
#cutting so only 4 clusters
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)

table(wisc.hclust.clusters, diagnosis)

```
Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

```{r}
#cutting so only 4 clusters
wisc.hclust.clusterz <- cutree(wisc.hclust, k = 2)

table(wisc.hclust.clusterz, diagnosis)

```

Q12. Can you find a better cluster v diagnosis match by changing the # of clusters?
Observing 2 clusters seems like a good match to observe. Less groups give us a clearer image of benign vs malignant.


```{r}
wisc.hclustzz <- hclust(data.dist, method = "ward.D2")
plot(wisc.hclustzz)
```
Q13. Favorite method?
I think I like ward.D2 as well. Minimizing the variance creates such a clean and aesthetically pleasing picture to observe and understand.


```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method= "ward.D2")

plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```
```{r}
table(grps, diagnosis)
```
```{r}
plot(wisc.pr$x[,1:2], col=grps)
```
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "ward.D2")

wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k = 2)
```


```{r}
#compare diagnosis
table(wisc.pr.hclust.clusters, diagnosis)
```
Q15. How well does the newly created model with four clusters seperate out the two diagnosis?
I think it works really well. We are able to get a clearer picture of the diagnosis data and they are grouped together efficiently.

```{r}
table(wisc.hclust.clusters, diagnosis)
```


Q16. How do the other models do in terms of serperating diagnosis?
H-cluster model does not work as well in seperating diagnoses. The model we just made is more clear, has less groups to interpret. It is good to note that the models had similar outputs in terms of data.

Q17. Which analysis procedures resulted in the best specificity? Best sensitivity?
I think the ward.D2 procedure had best specificity and sensitivity.

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```
```{r}
plot(wisc.pr$x[,1:2])
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```
Q18. Which of the new patients should be prioritized for follow up based on results?
Patient 2 needs to be followed up on for their results.

